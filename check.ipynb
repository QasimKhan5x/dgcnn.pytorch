{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from s3dis_util import crop_pc\n",
    "import s3dis_transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3DIS(Dataset):\n",
    "    classes = ['ceiling',\n",
    "               'floor',\n",
    "               'wall',\n",
    "               'beam',\n",
    "               'column',\n",
    "               'window',\n",
    "               'door',\n",
    "               'chair',\n",
    "               'table',\n",
    "               'bookcase',\n",
    "               'sofa',\n",
    "               'board',\n",
    "               'clutter']\n",
    "    num_classes = 13\n",
    "    num_per_class = np.array([3370714, 2856755, 4919229, 318158, 375640, 478001, 974733,\n",
    "                              650464, 791496, 88727, 1284130, 229758, 2272837], dtype=np.int32)\n",
    "    class2color = {'ceiling':     [0, 255, 0],\n",
    "                   'floor':       [0, 0, 255],\n",
    "                   'wall':        [0, 255, 255],\n",
    "                   'beam':        [255, 255, 0],\n",
    "                   'column':      [255, 0, 255],\n",
    "                   'window':      [100, 100, 255],\n",
    "                   'door':        [200, 200, 100],\n",
    "                   'table':       [170, 120, 200],\n",
    "                   'chair':       [255, 0, 0],\n",
    "                   'sofa':        [200, 100, 100],\n",
    "                   'bookcase':    [10, 200, 100],\n",
    "                   'board':       [200, 200, 200],\n",
    "                   'clutter':     [50, 50, 50]}\n",
    "    cmap = [*class2color.values()]\n",
    "    \"\"\"S3DIS dataset, loading the subsampled entire room as input without block/sphere subsampling.\n",
    "    Args:\n",
    "        data_root (str, optional): Defaults to 'data/S3DIS/s3disfull'.\n",
    "        test_area (int, optional): Defaults to 5.\n",
    "        voxel_size (float, optional): the voxel size for donwampling. Defaults to 0.04.\n",
    "        voxel_max (_type_, optional): subsample the max number of point per point cloud. Set None to use all points.  Defaults to None.\n",
    "        split (str, optional): Defaults to 'train'.\n",
    "        transform (_type_, optional): Defaults to None.\n",
    "        loop (int, optional): split loops for each epoch. Defaults to 1.\n",
    "        presample (bool, optional): wheter to downsample each point cloud before training. Set to False to downsample on-the-fly. Defaults to True.\n",
    "        variable (bool, optional): where to use the original number of points. The number of point per point cloud is variable. Defaults to False.\n",
    "        n_shifted (int, optional): the number of shifted coordinates to be used. Defaults to 1 to use the height.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_root: str = 'data/S3DIS/s3disfull',\n",
    "                 test_area: int = 5,\n",
    "                 voxel_size: float = 0.04,\n",
    "                 voxel_max=None,\n",
    "                 split: str = 'train',\n",
    "                 transform=None,\n",
    "                 loop: int = 1,\n",
    "                 presample: bool = False,\n",
    "                 variable: bool = False,\n",
    "                 n_shifted: int = 1,\n",
    "                 append_height: bool = True\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.split, self.voxel_size, self.transform, self.voxel_max, self.loop = \\\n",
    "            split, voxel_size, transform, voxel_max, loop\n",
    "        self.presample = presample\n",
    "        self.variable = variable\n",
    "        self.n_shifted = n_shifted\n",
    "        self.append_height = append_height\n",
    "\n",
    "        raw_root = os.path.join(data_root, 'raw')\n",
    "        self.raw_root = raw_root\n",
    "        data_list = sorted(os.listdir(raw_root))\n",
    "        data_list = [item[:-4] for item in data_list if 'Area_' in item]\n",
    "        if split == 'train':\n",
    "            self.data_list = [\n",
    "                item for item in data_list if not 'Area_{}'.format(test_area) in item]\n",
    "        else:\n",
    "            self.data_list = [\n",
    "                item for item in data_list if 'Area_{}'.format(test_area) in item]\n",
    "\n",
    "        processed_root = os.path.join(data_root, 'processed')\n",
    "        filename = os.path.join(\n",
    "            processed_root, f's3dis_{split}_area{test_area}_{voxel_size:.3f}.pkl')\n",
    "        if presample and not os.path.exists(filename):\n",
    "            np.random.seed(0)\n",
    "            self.data = []            \n",
    "            for item in tqdm(self.data_list, desc=f'Loading S3DISFull {split} split on Test Area {test_area}'):\n",
    "                data_path = os.path.join(raw_root, item + '.npy')\n",
    "                cdata = np.load(data_path).astype(np.float32)\n",
    "                cdata[:, :3] -= np.min(cdata[:, :3], 0)\n",
    "                if voxel_size is not None:\n",
    "                    coord, feat, label = cdata[:,\n",
    "                                               0:3], cdata[:, 3:6], cdata[:, 6:7]\n",
    "                    coord, feat, label = crop_pc(\n",
    "                        coord, feat, label, self.split, self.voxel_size, self.voxel_max,\n",
    "                        downsample=not self.presample, variable=self.variable)\n",
    "                    cdata = np.hstack((coord, feat, label))\n",
    "                self.data.append(cdata)\n",
    "            npoints = np.array([len(data) for data in self.data])\n",
    "            logging.info('split: %s, median npoints %.1f, avg num points %.1f, std %.1f' % (\n",
    "                self.split, np.median(npoints), np.average(npoints), np.std(npoints)))\n",
    "            os.makedirs(processed_root, exist_ok=True)\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(self.data, f)\n",
    "                print(f\"{filename} saved successfully\")\n",
    "        elif presample:\n",
    "            with open(filename, 'rb') as f:\n",
    "                self.data = pickle.load(f)\n",
    "                print(f\"{filename} load successfully\")\n",
    "        self.data_idx = np.arange(len(self.data_list))\n",
    "        assert len(self.data_idx) > 0\n",
    "        logging.info(f\"\\nTotally {len(self.data_idx)} samples in {split} set\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_idx = self.data_idx[idx % len(self.data_idx)]\n",
    "        if self.presample:\n",
    "            coord, feat, label = np.split(self.data[data_idx], [3, 6], axis=1)\n",
    "        else:\n",
    "            data_path = os.path.join(\n",
    "                self.raw_root, self.data_list[data_idx] + '.npy')\n",
    "            cdata = np.load(data_path).astype(np.float32)\n",
    "            cdata[:, :3] -= np.min(cdata[:, :3], 0)\n",
    "            coord, feat, label = cdata[:, :3], cdata[:, 3:6], cdata[:, 6:7]\n",
    "            coord, feat, label = crop_pc(\n",
    "                coord, feat, label, self.split, self.voxel_size, self.voxel_max,\n",
    "                downsample=not self.presample, variable=self.variable)\n",
    "        label = label.squeeze(-1).astype(np.int_)\n",
    "        data = {'pos': coord, 'x': feat, 'y': label}\n",
    "        # augmentation\n",
    "        if self.transform is not None:\n",
    "            data['pos'], data['x'], data['y'] = self.transform(\n",
    "                data['pos'], data['x'], data['y'])\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_idx) * self.loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/S3DIS/s3disfull/processed/s3dis_train_area5_0.040.pkl load successfully\n",
      "data/S3DIS/s3disfull/processed/s3dis_val_area5_0.040.pkl load successfully\n"
     ]
    }
   ],
   "source": [
    "train_ds = S3DIS(split='train', voxel_max=24000, presample=True, transform=T.Compose([\n",
    "    T.PointCloudFloorCentering(), T.AppendHeight(),\n",
    "    T.RandomScale(), T.RandomRotate(), T.RandomJitter(),\n",
    "    T.ChromaticNormalize(), T.ChromaticAutoContrast(),\n",
    "    T.RandomDropColor(), T.ToTensor()\n",
    "]))\n",
    "test_ds = S3DIS(split='val', presample=True, transform=T.Compose([\n",
    "    T.PointCloudFloorCentering(), T.AppendHeight(),\n",
    "    T.ChromaticNormalize(), T.ToTensor()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(test_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dataloader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
