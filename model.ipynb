{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients and GCN Features Fusion Transformer for Point Cloud Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37m8d809b5da21a       \u001b[m  Tue Aug 16 07:33:26 2022  \u001b[1m\u001b[30m460.73.01\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 68'C\u001b[m, \u001b[1m\u001b[32m 92 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m10241\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[1]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 59'C\u001b[m, \u001b[1m\u001b[32m 59 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m13846\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[2]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[31m 44'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m10325\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[3]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 54'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m15031\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[4]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[31m 43'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 1287\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[5]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 56'C\u001b[m, \u001b[1m\u001b[32m 86 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m23739\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[6]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 55'C\u001b[m, \u001b[1m\u001b[32m 91 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m23739\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[7]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[31m 30'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    8\u001b[m / \u001b[33m24268\u001b[m MB |\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = [0]\n",
    "dev = None if len(available_gpus) == 0 else available_gpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from data import ShapeNetPart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_mem(*objs):\n",
    "    for obj in objs:\n",
    "        del obj\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(x, k):\n",
    "    inner = -2*torch.matmul(x.transpose(2, 1), x)\n",
    "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
    "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
    "    # (batch_size, num_points, k)\n",
    "    idx = pairwise_distance.topk(k=k, dim=-1)[1]\n",
    "    return idx\n",
    "\n",
    "\n",
    "def get_graph_feature(x, k, knn_only=False, disp_only=False):\n",
    "    batch_size = x.size(0)\n",
    "    num_points = x.size(2)\n",
    "    x = x.view(batch_size, -1, num_points)\n",
    "    idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
    "    device = x.get_device()\n",
    "    idx_base = torch.arange(0, batch_size).view(-1, 1, 1) * num_points\n",
    "    idx_base = idx_base.cuda(device)\n",
    "    idx = idx + idx_base\n",
    "    idx = idx.view(-1)\n",
    "    _, num_dims, _ = x.size()\n",
    "    # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
    "    x = x.transpose(2, 1).contiguous()\n",
    "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims)\n",
    "    if knn_only:\n",
    "        return feature\n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "    if disp_only:\n",
    "        return (feature - x).permute(0, 3, 1, 2)\n",
    "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "    return feature      # (batch_size, 2*num_dims, num_points, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform_Net(nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super(Transform_Net, self).__init__()\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn1,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=1, bias=False),\n",
    "                                   self.bn2,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(128, 1024, kernel_size=1, bias=False),\n",
    "                                   self.bn3,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "\n",
    "        self.linear1 = nn.Linear(1024, 512, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.linear2 = nn.Linear(512, 256, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.transform = nn.Linear(256, 3*3)\n",
    "        nn.init.constant_(self.transform.weight, 0)\n",
    "        nn.init.eye_(self.transform.bias.view(3, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (B x 3 x N)\n",
    "        batch_size = x.size(0)\n",
    "        # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n",
    "        t = get_graph_feature(x, k=self.k)\n",
    "        \n",
    "        t = self.conv1(t)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        t = self.conv2(t)                       # (batch_size, 64, num_points, k) -> (batch_size, 128, num_points, k)\n",
    "        t = t.max(dim=-1, keepdim=False)[0]     # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n",
    "        \n",
    "        t = self.conv3(t)                       # (batch_size, 128, num_points) -> (batch_size, 1024, num_points)\n",
    "        t = t.max(dim=-1, keepdim=False)[0]     # (batch_size, 1024, num_points) -> (batch_size, 1024)\n",
    "        \n",
    "        t = F.leaky_relu(self.bn4(self.linear1(t)), negative_slope=0.2)     # (batch_size, 1024) -> (batch_size, 512)\n",
    "        t = F.leaky_relu(self.bn5(self.linear2(t)), negative_slope=0.2)     # (batch_size, 512) -> (batch_size, 256)\n",
    "        \n",
    "        t = self.transform(t)                   # (batch_size, 256) -> (batch_size, 3*3)\n",
    "        t = t.view(batch_size, 3, 3)            # (batch_size, 3*3) -> (batch_size, 3, 3)\n",
    "        \n",
    "        # (batch_size, 3, num_points) -> (batch_size, num_points, 3)\n",
    "        x = x.transpose(2, 1)\n",
    "        # (batch_size, num_points, 3) * (batch_size, 3, 3) -> (batch_size, num_points, 3)\n",
    "        x = torch.bmm(x, t)\n",
    "        # (batch_size, num_points, 3) -> (batch_size, 3, num_points)\n",
    "        x = x.transpose(2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCNN(nn.Module):\n",
    "    def __init__(self, k, emb_dim):\n",
    "        super(DGCNN, self).__init__()\n",
    "        self.k = k\n",
    "        self.emb_dim = emb_dim\n",
    "    \n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv6 = nn.Sequential(nn.Conv1d(192, emb_dim, kernel_size=1, bias=False),\n",
    "                                   nn.BatchNorm1d(emb_dim),\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        num_points = x.size(2)\n",
    "\n",
    "        x = get_graph_feature(x, k=self.k)      # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n",
    "        x = self.conv1(x)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x = self.conv2(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x1 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "\n",
    "        x = get_graph_feature(x1, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
    "        x = self.conv3(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x = self.conv4(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x2 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "\n",
    "        x = get_graph_feature(x2, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n",
    "        x = self.conv5(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n",
    "        x3 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), dim=1)      # (batch_size, 64*3, num_points)\n",
    "        x = self.conv6(x)                       # (batch_size, 64*3, num_points) -> (batch_size, emb_dims, num_points)\n",
    "        # (batch_size, num_points, emb_dims)\n",
    "        y = x.view(batch_size, num_points, self.emb_dim)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients & HOG 3D\n",
    "\n",
    "HOG is not implemented yet. Will implement later after creating pipeline with SVD only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients(x, k, do_pca=False):\n",
    "    '''\n",
    "    x (Bx3xN) batch of point clouds\n",
    "    return gradients (BxNx3): direction of maximimal variance at each point\n",
    "    '''\n",
    "    x_nn = get_graph_feature(x, k=k, knn_only=True)  # Bx3xN -> BxNxkx3\n",
    "    if do_pca:\n",
    "        _, _, v = torch.pca_lowrank(x_nn)  # BxNxkx3 -> BxNx3x3\n",
    "        \n",
    "    else:\n",
    "        mean = x_nn.mean(dim=2).unsqueeze(dim=2)\n",
    "        centered = x_nn - mean\n",
    "        _, _, v = torch.linalg.svd(centered)  # BxNxkx3 -> BxNx3x3\n",
    "    gradients = v[:, :, 0]  # BxNx3x3 -> BxNx3\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_angle_histogram(gradients, choice=\"cartesian\"):\n",
    "#     '''\n",
    "#     Takes input gradients (BxNx3)\n",
    "#     Computes angles in cartesian or spherical coordinates\n",
    "#     Returns an array (Bx180x3 or Bx100x2) of angle frequencie\n",
    "#     '''\n",
    "#     assert choice in (\"cartesian\", \"spherical\")\n",
    "#     # gradients = gradients.cpu()\n",
    "#     batch_size = gradients.size(0)\n",
    "#     if choice == \"cartesian\":\n",
    "#         angles = torch.acos(gradients) * 180.0 / math.pi\n",
    "#         freq_table = torch.zeros((batch_size, 180, 3))\n",
    "#         for i in range(batch_size):\n",
    "#             for j in range(3):\n",
    "#                 freq_table[i, :, j] = torch.histc(\n",
    "#                     angles[i, :, j], bins=180, min=0, max=180)\n",
    "#     else:\n",
    "#         angles = torch.empty((gradients.size(0), gradients.size(1), 2))\n",
    "#         # theta\n",
    "#         angles[:, :, 0] = torch.atan(gradients[:, :, 1] / gradients[:, :, 0])\n",
    "#         # pi\n",
    "#         angles[:, :, 1] = torch.acos(gradients[:, :, 2])\n",
    "#         angles = angles * 180 / math.pi\n",
    "#         # range is [-100,100]\n",
    "#         freq_table = torch.zeros((batch_size, 100, 2))\n",
    "#         for i in range(batch_size):\n",
    "#             freq_table[i, :, 0] = torch.histc(angles[i, :, 0])\n",
    "#             freq_table[i, :, 1] = torch.histc(angles[i, :, 1])\n",
    "#     # get density instead of count\n",
    "#     return freq_table / 2048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs = 10\n",
    "# n = 2048\n",
    "\n",
    "# train_dataset = ShapeNetPart(n, 'trainval')\n",
    "# train_loader = DataLoader(train_dataset, num_workers=2, batch_size=bs,\n",
    "#                           shuffle=True, drop_last=False)\n",
    "\n",
    "# k = 20\n",
    "\n",
    "# for data, _, _ in train_loader:\n",
    "#     print(data.shape)\n",
    "#     data = data.cuda().permute(0, 2, 1)\n",
    "#     print(data.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradients = get_gradients(data, k=20)\n",
    "# hist = get_angle_histogram(gradients)\n",
    "# for i in range(bs):\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.bar(range(180), hist[i, :, 0], alpha=0.5)\n",
    "#     plt.bar(range(180), hist[i, :, 1], alpha=0.5)\n",
    "#     plt.bar(range(180), hist[i, :, 2], alpha=0.5)\n",
    "#     plt.legend(labels=['x', 'y', 'z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = get_angle_histogram(gradients, 'spherical')\n",
    "# for i in range(bs):\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.bar(range(100), hist[i, :, 0], alpha=0.5)\n",
    "#     plt.bar(range(100), hist[i, :, 1], alpha=0.5)\n",
    "#     plt.legend(labels=['theta', 'phi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Point Transformer**\n",
    "\n",
    "1. Decrease `emb_dims` by 1x1 conv like DETR\n",
    "2. Use attention mech of Point Transformer (Hengshuang)\n",
    "3. Resize to original dim\n",
    "3. Design encoder of original transformer (without multihead for now)\n",
    "\n",
    "**PointBERT**\n",
    "\n",
    "- Patch based embeddings... (1 embedding for 1 patch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Transformer by [qq456cvb](https://github.com/qq456cvb/Point-Transformers/blob/master/models/Hengshuang/transformer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_distance(src, dst):\n",
    "    return torch.sum((src[:, :, None] - dst[:, None]) ** 2, dim=-1)\n",
    "\n",
    "\n",
    "def index_points(points, idx):\n",
    "    raw_size = idx.size()\n",
    "    idx = idx.reshape(raw_size[0], -1)\n",
    "    res = torch.gather(\n",
    "        points, 1, idx[..., None].expand(-1, -1, points.size(-1)))\n",
    "    return res.reshape(*raw_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointTransformerLayer(nn.Module):\n",
    "    def __init__(self, d_points=256, d_model=64, k=16) -> None:\n",
    "        super(PointTransformerLayer, self).__init__()\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "        self.fc1 = nn.Linear(d_points, d_model)\n",
    "        self.fc2 = nn.Linear(d_model, d_points)\n",
    "\n",
    "        self.fc_delta = nn.Sequential(\n",
    "            nn.Linear(3, d_model, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        self.fc_gamma = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        self.w_qs = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, xyz, features):\n",
    "        # xyz: b x n x 3, features: b x n x f\n",
    "        dists = square_distance(xyz, xyz)\n",
    "        knn_idx = dists.argsort()[:, :, :self.k]  # b x n x k\n",
    "        knn_xyz = index_points(xyz, knn_idx)\n",
    "        \n",
    "        pre = features\n",
    "        x = self.fc1(features)\n",
    "        q, k, v = self.w_qs(x), index_points(self.w_ks(x), knn_idx), index_points(self.w_vs(x), knn_idx)\n",
    "\n",
    "        pos_enc = self.fc_delta(xyz[:, :, None] - knn_xyz)  # b x n x k x f\n",
    "        \n",
    "        attn = self.fc_gamma(q[:, :, None] - k + pos_enc)\n",
    "        attn = F.softmax(attn, dim=-2)  # b x n x k x f\n",
    "        attn = F.normalize(attn, p=1.0, dim=-2)\n",
    "        \n",
    "        res = torch.einsum('bmnf,bmnf->bmf', attn, v + pos_enc)\n",
    "        res = self.fc2(res) + pre\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module)] * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/POSTECH-CVLab/point-transformer/blob/master/model/pointtransformer/pointtransformer_seg.py\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 mid_channels=256,\n",
    "                 out_channels=None,\n",
    "                 ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # output has same dim has input\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        # only use for large dim inputs\n",
    "        mid_channels = mid_channels if in_channels > mid_channels else in_channels\n",
    "\n",
    "        self.scale_dim = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, mid_channels, 1, bias=False),\n",
    "            nn.BatchNorm1d(mid_channels),\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "\n",
    "        self.attention = PointTransformerLayer(d_points=mid_channels)\n",
    "        self.bn = nn.BatchNorm1d(mid_channels)\n",
    "\n",
    "        self.restore_dim = nn.Sequential(\n",
    "            nn.Conv1d(mid_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "\n",
    "        self.skip_conn = nn.Sequential(\n",
    "            nn.LayerNorm(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "            \n",
    "\n",
    "    def forward(self, p, x):\n",
    "        # x (B x N x C)\n",
    "        # p (B x N x 3)\n",
    "\n",
    "        bs = x.size(0)\n",
    "        n = x.size(1)\n",
    "\n",
    "        # x -> B x C x N -> B x mid_channels x N\n",
    "        y = self.scale_dim(x.transpose(1, 2))\n",
    "        # p (B x N x 3) & y (B x mid_channels x N) -> (B x mid_channels x N )\n",
    "        y = F.relu(self.bn(self.attention(\n",
    "            p, y.transpose(1, 2)).transpose(1, 2)))\n",
    "        # (B x N x mid_channels) -> (B x out_channels x N) -> (B x N x out_channels)\n",
    "\n",
    "        y = self.restore_dim(y)\n",
    "        y = y.transpose(1, 2)\n",
    "        # skip connection (won't work if in_channels != out_channels)\n",
    "        y = y + x\n",
    "        y = self.skip_conn(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.layers = _get_clones(EncoderLayer(in_channels=in_channels, **kwargs), num_layers)\n",
    "        self.norm = nn.LayerNorm(in_channels)\n",
    "\n",
    "    def forward(self, pc, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(pc, x)\n",
    "        y = self.norm(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    Self attention with multiple heads\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d_graph, d_grads, d_k, num_heads, d_v=None,dropout=.1):\n",
    "        '''\n",
    "        :param d_model: Output dimensionality of the model\n",
    "        :param d_k: Dimensionality of queries and keys\n",
    "        :param d_v: Dimensionality of values\n",
    "        :param h: Number of heads\n",
    "        '''\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.d_graph = d_graph\n",
    "        self.d_grads = d_grads\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_k if d_v is None else d_v\n",
    "        self.h = num_heads\n",
    "\n",
    "        self.fc_q = nn.Linear(self.d_grads, self.h * self.d_k)\n",
    "        self.fc_k = nn.Linear(self.d_graph, self.h * self.d_k)\n",
    "        self.fc_v = nn.Linear(self.d_graph, self.h * self.d_v)\n",
    "        self.fc_o = nn.Linear(self.h * self.d_v, self.d_graph)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None, attention_weights=None):\n",
    "        '''\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_grads)\n",
    "        :param keys: Keys (b_s, nk, d_graph)\n",
    "        :param values: Values (b_s, nk, d_graph)\n",
    "        :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.\n",
    "        :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).\n",
    "        :return:\n",
    "        '''\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "\n",
    "        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "        if attention_weights is not None:\n",
    "            att = att * attention_weights\n",
    "        if attention_mask is not None:\n",
    "            att = att.masked_fill(attention_mask, -np.inf)\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = F.normalize(att, p=1.0, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_graph)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Offset_Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels_graph, in_channels_grads, \n",
    "                 mid_channels=64, out_channels=None, \n",
    "                 num_heads=8, dropout=0.1):\n",
    "        super(Offset_Attention, self).__init__()\n",
    "\n",
    "        if out_channels is None:\n",
    "            out_channels = in_channels_graph\n",
    "\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, d_graph=in_channels_graph,\n",
    "                                            d_grads=in_channels_grads, d_k=mid_channels, \n",
    "                                            dropout=dropout)\n",
    "        \n",
    "        self.lbr = nn.Sequential(\n",
    "            nn.Conv1d(out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, grads, graph):\n",
    "        # grads (B x N x 3)\n",
    "        # graph (B x N x F)\n",
    "        attn_output = self.attention(queries=grads, keys=graph, values=graph)\n",
    "        attn_output = attn_output - graph\n",
    "        y = self.lbr(attn_output.transpose(1, 2)).transpose(1, 2)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "- Pointformer LGT block \"*adopts a multi-scale cross-attention module to build connections between local features ... and global features*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, nheads, nlayers, n_classes) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nheads,\n",
    "                                           batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(layer, num_layers=nlayers)\n",
    "        \n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        # tgt (B x N x F) memory (B x N x F)\n",
    "        tgt = self.decoder(tgt, memory)  # (B x N x F)\n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, d_model, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_model // 8, 1, bias=False),\n",
    "            nn.BatchNorm1d(d_model // 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv1d(d_model // 8, d_model // 64, 1, bias=False),\n",
    "            nn.BatchNorm1d(d_model // 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv1d(d_model // 64, n_classes, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (B x N x F)\n",
    "        return self.clf(x.transpose(1, 2))  # (B x n_classes x N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, k, emb_dim, nlayers, nclasses):\n",
    "        super(Net, self).__init__()\n",
    "        # number of nearest neighbors\n",
    "        self.k = k\n",
    "        # transform to canonical representation\n",
    "        self.tnet = Transform_Net(k=k)\n",
    "        self.tnet.load_state_dict(torch.load(\"ckpts/tnet.pt\"))\n",
    "        # get graph features\n",
    "        self.dgcnn = DGCNN(k=k, emb_dim=emb_dim)\n",
    "        self.dgcnn.load_state_dict(torch.load(\"ckpts/dgcnn.pt\"))\n",
    "        # produce attn_map from graph features\n",
    "        self.graph_encoder = TransformerEncoder(\n",
    "            in_channels=emb_dim, num_layers=nlayers)\n",
    "        # produce attn_map from gradients\n",
    "        self.gradients_encoder = TransformerEncoder(\n",
    "            in_channels=3, num_layers=nlayers)\n",
    "        # fuse gradients and graph attention\n",
    "        self.fusion_net = Offset_Attention(in_channels_graph=emb_dim, in_channels_grads=3)\n",
    "        # get segmap\n",
    "        # self.decoder = Decoder(d_model=emb_dim, nheads=4,\n",
    "        #                        nlayers=nlayers, n_classes=50)\n",
    "        self.clf = Classifier(d_model=emb_dim, n_classes=nclasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = B x 3 x N\n",
    "        pcd = self.tnet(x) # B x 3 x N\n",
    "        # B x 3 x N -> B x N x emb_dim\n",
    "        graph_ftrs = self.dgcnn(pcd)\n",
    "        # B x N x 3\n",
    "        gradient_ftrs = get_gradients(x, k=self.k)\n",
    "        # B x 3 x N -> B x N x 3\n",
    "        pcd = pcd.transpose(1, 2)\n",
    "        # (B x N x 3) & (B x N x emb_dim) -> B x N x emb_dim\n",
    "        graph_attn_map = self.graph_encoder(pcd, graph_ftrs)\n",
    "        # (B x N x 3) & (B x N x 3) -> B x N x 3\n",
    "        gradients_attn_map = self.gradients_encoder(pcd, gradient_ftrs)\n",
    "        # (B x N x 3) & (B x N x emb_dim) -> B x N x emb_dim\n",
    "        fused_attn_map = self.fusion_net(gradients_attn_map, graph_attn_map)\n",
    "        # (B x N x emb_dim) & (B x N x emb_dim) -> B x n_classes x N\n",
    "        # segmap = self.decoder(graph_ftrs, fused_attn_map)\n",
    "        scores = self.clf(fused_attn_map)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "emb_dim = 1024\n",
    "k = 16\n",
    "dev = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(k=k, emb_dim=emb_dim, nlayers=4, nclasses=50).cuda(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4402428"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37ma1aa4d27e2e8       \u001b[m  Fri Aug 12 16:14:06 2022  \u001b[1m\u001b[30m460.73.01\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 67'C\u001b[m, \u001b[1m\u001b[32m 95 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m13287\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[1]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[31m 49'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m11669\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[2]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[31m 45'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m10325\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[3]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 63'C\u001b[m, \u001b[1m\u001b[32m 82 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m21144\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[4]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[31m 44'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 1287\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[5]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 60'C\u001b[m, \u001b[1m\u001b[32m 40 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m23833\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[6]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 64'C\u001b[m, \u001b[1m\u001b[32m 91 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m17893\u001b[m / \u001b[33m24268\u001b[m MB |\n",
      "\u001b[36m[7]\u001b[m \u001b[34mGeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 65'C\u001b[m, \u001b[1m\u001b[32m 82 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m19527\u001b[m / \u001b[33m24268\u001b[m MB |\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn((2, 2048, 3), device=torch.device(dev)).transpose(1, 2)\n",
    "# y = model(x)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignite Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import ConfusionMatrix, IoU, Loss, mIoU\n",
    "from ignite.handlers import ModelCheckpoint, global_step_from_engine\n",
    "from ignite.contrib.handlers import TensorboardLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShapeNetPart(2048, 'train', task='seg')\n",
    "N = len(dataset)\n",
    "\n",
    "train_ds = Subset(dataset, list(range(2)))\n",
    "val_ds = Subset(dataset, [i for i in range(N-2, N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(k=16, emb_dim=1024, ).to(device)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=5e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "miou = JaccardIndex(50).cuda(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 0.9777383208274841  mIOU: 0.0007681986317038536\n",
      "10 Loss: 0.2872034013271332  mIOU: 0.005980056244879961\n",
      "20 Loss: 0.26222798228263855  mIOU: 0.00602493854239583\n",
      "30 Loss: 0.2574349641799927  mIOU: 0.00583995645865798\n",
      "40 Loss: 0.2558836042881012  mIOU: 0.005877274088561535\n",
      "50 Loss: 0.25441649556159973  mIOU: 0.0059401593171060085\n",
      "60 Loss: 0.25282928347587585  mIOU: 0.005973484832793474\n",
      "70 Loss: 0.2522369623184204  mIOU: 0.005732319783419371\n",
      "80 Loss: 0.25165092945098877  mIOU: 0.005835120566189289\n",
      "90 Loss: 0.25097793340682983  mIOU: 0.005756357219070196\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1354735/2235433650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0my_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1354735/1161816250.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mfused_attn_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfusion_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients_attn_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_attn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# (B x N x emb_dim) & (B x N x emb_dim) -> B x n_classes x N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0msegmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_ftrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfused_attn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msegmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1354735/2897499865.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B x N x F)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# (B x N x F) -> (B x F x N ) -> (B x n_classes x N)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0msegmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msegmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    296\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    297\u001b[0m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0;32m--> 298\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    total_loss, total_iou = 0, 0\n",
    "    for b, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.cuda(dev), y.cuda(dev)\n",
    "        y_out = model(x)\n",
    "        loss = criterion(y_out, y)\n",
    "        total_loss += loss\n",
    "        preds = torch.argmax(y_out, dim=1)\n",
    "        total_iou += miou(preds, y)\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"{epoch} Loss:\", (total_loss / 4).item(),\n",
    "              \" mIOU:\", (total_iou / 4).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = create_supervised_trainer(model, optimizer, criterion, device)\n",
    "\n",
    "cm_metric = ConfusionMatrix(num_classes=50)\n",
    "val_metrics = {\"IoU\": mIoU(cm_metric), \"loss\": Loss(criterion)}\n",
    "\n",
    "train_evaluator = create_supervised_evaluator(\n",
    "    model, metrics=val_metrics, device=device)\n",
    "val_evaluator = create_supervised_evaluator(\n",
    "    model, metrics=val_metrics, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many batches to wait before logging training status\n",
    "log_interval = 2\n",
    "\n",
    "\n",
    "@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "def log_training_loss(engine):\n",
    "    print(\n",
    "        f\"Epoch[{engine.state.epoch}], Iter[{engine.state.iteration}] Loss: {engine.state.output:.2f}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    train_evaluator.run(train_loader)\n",
    "    metrics = train_evaluator.state.metrics\n",
    "    print(\n",
    "        f\"Training Results - Epoch[{trainer.state.epoch}] Avg IoU: {metrics['IoU']:.2f} Avg loss: {metrics['loss']:.2f}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(trainer):\n",
    "    val_evaluator.run(val_loader)\n",
    "    metrics = val_evaluator.state.metrics\n",
    "    print(\n",
    "        f\"Validation Results - Epoch[{trainer.state.epoch}] Avg IoU: {metrics['IoU']:.2f} Avg loss: {metrics['loss']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.engine.events.RemovableEventHandle at 0x7f2c1c3d8d90>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score function to return current value of any metric we defined above in val_metrics\n",
    "def score_function(engine):\n",
    "    return engine.state.metrics[\"IoU\"]\n",
    "\n",
    "\n",
    "# Checkpoint to store n_saved best models wrt score function\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"checkpoint\",\n",
    "    n_saved=2,\n",
    "    filename_prefix=\"best\",\n",
    "    score_function=score_function,\n",
    "    score_name=\"IoU\",\n",
    "    require_empty=False,\n",
    "    global_step_transform=global_step_from_engine(\n",
    "        trainer),  # helps fetch the trainer's state\n",
    ")\n",
    "\n",
    "# Save the model after every epoch of val_evaluator is completed\n",
    "val_evaluator.add_event_handler(\n",
    "    Events.COMPLETED, model_checkpoint, {\"model\": model})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Tensorboard logger\n",
    "tb_logger = TensorboardLogger(log_dir=\"tb-logger\")\n",
    "\n",
    "# Attach handler to plot trainer's loss every 100 iterations\n",
    "tb_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.ITERATION_COMPLETED(every=100),\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda loss: {\"batch_loss\": loss},\n",
    ")\n",
    "\n",
    "# Attach handler for plotting both evaluators' metrics after every epoch completes\n",
    "for tag, evaluator in [(\"training\", train_evaluator), (\"validation\", val_evaluator)]:\n",
    "    tb_logger.attach_output_handler(\n",
    "        evaluator,\n",
    "        event_name=Events.EPOCH_COMPLETED,\n",
    "        tag=tag,\n",
    "        metric_names=\"all\",\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results - Epoch[1] Avg IoU: 0.00 Avg loss: 2.88\n",
      "Validation Results - Epoch[1] Avg IoU: 0.00 Avg loss: 3.32\n",
      "Epoch[2], Iter[2] Loss: 3.39\n",
      "Training Results - Epoch[2] Avg IoU: 0.01 Avg loss: 15.34\n",
      "Validation Results - Epoch[2] Avg IoU: 0.01 Avg loss: 13.66\n",
      "Training Results - Epoch[3] Avg IoU: 0.00 Avg loss: 10.26\n",
      "Validation Results - Epoch[3] Avg IoU: 0.00 Avg loss: 20.52\n",
      "Epoch[4], Iter[4] Loss: 3.13\n",
      "Training Results - Epoch[4] Avg IoU: 0.00 Avg loss: 3.28\n",
      "Validation Results - Epoch[4] Avg IoU: 0.00 Avg loss: 4.41\n",
      "Training Results - Epoch[5] Avg IoU: 0.00 Avg loss: 3.85\n",
      "Validation Results - Epoch[5] Avg IoU: 0.00 Avg loss: 3.92\n",
      "Epoch[6], Iter[6] Loss: 2.05\n",
      "Training Results - Epoch[6] Avg IoU: 0.00 Avg loss: 5.07\n",
      "Validation Results - Epoch[6] Avg IoU: 0.00 Avg loss: 13.04\n",
      "Training Results - Epoch[7] Avg IoU: 0.02 Avg loss: 1.33\n",
      "Validation Results - Epoch[7] Avg IoU: 0.02 Avg loss: 5.44\n",
      "Epoch[8], Iter[8] Loss: 1.41\n",
      "Training Results - Epoch[8] Avg IoU: 0.02 Avg loss: 1.13\n",
      "Validation Results - Epoch[8] Avg IoU: 0.02 Avg loss: 4.11\n",
      "Training Results - Epoch[9] Avg IoU: 0.02 Avg loss: 1.08\n",
      "Validation Results - Epoch[9] Avg IoU: 0.01 Avg loss: 4.49\n",
      "Epoch[10], Iter[10] Loss: 1.25\n",
      "Training Results - Epoch[10] Avg IoU: 0.02 Avg loss: 1.06\n",
      "Validation Results - Epoch[10] Avg IoU: 0.01 Avg loss: 4.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State:\n",
       "\titeration: 10\n",
       "\tepoch: 10\n",
       "\tepoch_length: 1\n",
       "\tmax_epochs: 10\n",
       "\toutput: 1.2469780445098877\n",
       "\tbatch: <class 'list'>\n",
       "\tmetrics: <class 'dict'>\n",
       "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
       "\tseed: <class 'NoneType'>\n",
       "\ttimes: <class 'dict'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(train_loader, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's close the logger and inspect our results\n",
    "tb_logger.close()\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard - -logdir = .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At last we can view our best models\n",
    "!ls checkpoints\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
